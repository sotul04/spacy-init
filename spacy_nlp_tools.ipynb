{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy NLP Tools Implementation\n",
    "\n",
    "This notebook demonstrates the implementation of 8 essential NLP processing tools using SpaCy:\n",
    "\n",
    "1. **Sentence Splitter** - Splits text into individual sentences\n",
    "2. **Tokenization** - Breaks text into individual tokens (words, punctuation)\n",
    "3. **Stemming** - Reduces words to their root form\n",
    "4. **Lemmatization** - Reduces words to their canonical/dictionary form\n",
    "5. **Entity Masking** - Identifies and masks named entities\n",
    "6. **POS Tagger** - Identifies parts of speech for each token\n",
    "7. **Phrase Chunking** - Groups tokens into meaningful phrases\n",
    "8. **Syntactic Parser** - Analyzes grammatical structure and dependencies\n",
    "\n",
    "We'll demonstrate these tools on a research paper abstract to show their practical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(f\"SpaCy version: {spacy.__version__}\")\n",
    "print(f\"Model loaded: {nlp.meta['name']} v{nlp.meta['version']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Research Paper Abstract\n",
    "\n",
    "We'll use this abstract from a paper on \"Attention Is All You Need\" (Transformer architecture) as our test text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research paper abstract - \"Attention Is All You Need\" by Vaswani et al.\n",
    "abstract = \"\"\"\n",
    "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks \n",
    "that include an encoder and a decoder. The best performing models also connect the encoder and decoder \n",
    "through an attention mechanism. We propose a new simple network architecture, the Transformer, based \n",
    "solely on attention mechanisms, dispensing with recurrence and convolution entirely. Experiments on two \n",
    "machine translation tasks show that these models are superior in quality while being more parallelizable \n",
    "and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 \n",
    "English-to-German translation task, improving over the existing best results by over 2 BLEU points. \n",
    "On the WMT 2014 English-to-French translation task, our model establishes a new state-of-the-art \n",
    "BLEU score of 41.8 after training for 3.5 days on eight P100 GPUs, a small fraction of the training \n",
    "costs of the best models described in the literature.\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"Research Paper Abstract:\")\n",
    "print(\"=\" * 50)\n",
    "print(abstract)\n",
    "print(\"\\nText length:\", len(abstract), \"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sentence Splitter\n",
    "\n",
    "The sentence splitter identifies sentence boundaries in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_splitter(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Split text into individual sentences using spaCy's sentence segmentation.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to split\n",
    "        nlp_model: Loaded spaCy model\n",
    "    \n",
    "    Returns:\n",
    "        list: List of sentences\n",
    "    \"\"\"\n",
    "    doc = nlp_model(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "# Apply sentence splitting\n",
    "sentences = sentence_splitter(abstract, nlp)\n",
    "\n",
    "print(\"Sentence Splitter Results:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Number of sentences: {len(sentences)}\\n\")\n",
    "\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization\n",
    "\n",
    "Tokenization breaks text into individual tokens (words, punctuation, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Tokenize text into individual tokens using spaCy.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to tokenize\n",
    "        nlp_model: Loaded spaCy model\n",
    "    \n",
    "    Returns:\n",
    "        list: List of token information dictionaries\n",
    "    \"\"\"\n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        tokens.append({\n",
    "            'text': token.text,\n",
    "            'is_alpha': token.is_alpha,\n",
    "            'is_punct': token.is_punct,\n",
    "            'is_space': token.is_space,\n",
    "            'is_stop': token.is_stop,\n",
    "            'shape': token.shape_\n",
    "        })\n",
    "    \n",
    "    return tokens, doc\n",
    "\n",
    "# Apply tokenization\n",
    "tokens, doc = tokenization(abstract, nlp)\n",
    "\n",
    "print(\"Tokenization Results:\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"Total tokens: {len(tokens)}\\n\")\n",
    "\n",
    "# Show first 20 tokens with details\n",
    "token_df = pd.DataFrame(tokens[:20])\n",
    "print(\"First 20 tokens:\")\n",
    "print(token_df.to_string(index=False))\n",
    "\n",
    "# Token statistics\n",
    "alpha_tokens = sum(1 for t in tokens if t['is_alpha'])\n",
    "punct_tokens = sum(1 for t in tokens if t['is_punct'])\n",
    "stop_tokens = sum(1 for t in tokens if t['is_stop'])\n",
    "\n",
    "print(f\"\\nToken Statistics:\")\n",
    "print(f\"Alphabetic tokens: {alpha_tokens}\")\n",
    "print(f\"Punctuation tokens: {punct_tokens}\")\n",
    "print(f\"Stop words: {stop_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stemming\n",
    "\n",
    "Note: SpaCy doesn't have built-in stemming, but we can implement basic stemming rules or use external libraries. Here we'll implement a simple rule-based stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_stemmer(word):\n",
    "    \"\"\"\n",
    "    Simple rule-based stemmer for demonstration.\n",
    "    In production, use libraries like NLTK's PorterStemmer.\n",
    "    \n",
    "    Args:\n",
    "        word (str): Word to stem\n",
    "    \n",
    "    Returns:\n",
    "        str: Stemmed word\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Common suffix removal rules\n",
    "    suffixes = [\n",
    "        ('ing', ''), ('ly', ''), ('ed', ''), ('ies', 'y'), ('ied', 'y'),\n",
    "        ('ies', 'y'), ('s', ''), ('es', ''), ('er', ''), ('est', ''),\n",
    "        ('tion', 'te'), ('ness', ''), ('ment', '')\n",
    "    ]\n",
    "    \n",
    "    for suffix, replacement in suffixes:\n",
    "        if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
    "            return word[:-len(suffix)] + replacement\n",
    "    \n",
    "    return word\n",
    "\n",
    "def stemming(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Apply stemming to tokens in text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        nlp_model: Loaded spaCy model\n",
    "    \n",
    "    Returns:\n",
    "        list: List of stemmed tokens\n",
    "    \"\"\"\n",
    "    doc = nlp_model(text)\n",
    "    stemmed_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_alpha and not token.is_stop:\n",
    "            stemmed = simple_stemmer(token.text)\n",
    "            stemmed_tokens.append({\n",
    "                'original': token.text,\n",
    "                'stemmed': stemmed,\n",
    "                'pos': token.pos_\n",
    "            })\n",
    "    \n",
    "    return stemmed_tokens\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_tokens = stemming(abstract, nlp)\n",
    "\n",
    "print(\"Stemming Results:\")\n",
    "print(\"=\" * 20)\n",
    "print(f\"Total stemmed tokens: {len(stemmed_tokens)}\\n\")\n",
    "\n",
    "# Show examples where stemming made a difference\n",
    "different_stems = [t for t in stemmed_tokens if t['original'].lower() != t['stemmed']]\n",
    "\n",
    "print(\"Examples where stemming changed the word:\")\n",
    "stem_df = pd.DataFrame(different_stems[:15])\n",
    "if not stem_df.empty:\n",
    "    print(stem_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No significant stemming changes found with simple rules.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Lemmatization\n",
    "\n",
    "Lemmatization reduces words to their canonical dictionary form using linguistic knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Apply lemmatization to tokens using spaCy's built-in lemmatizer.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        nlp_model: Loaded spaCy model\n",
    "    \n",
    "    Returns:\n",
    "        list: List of lemmatized tokens with metadata\n",
    "    \"\"\"\n",
    "    doc = nlp_model(text)\n",
    "    lemmatized_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            lemmatized_tokens.append({\n",
    "                'original': token.text,\n",
    "                'lemma': token.lemma_,\n",
    "                'pos': token.pos_,\n",
    "                'is_stop': token.is_stop\n",
    "            })\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized_tokens = lemmatization(abstract, nlp)\n",
    "\n",
    "print(\"Lemmatization Results:\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"Total lemmatized tokens: {len(lemmatized_tokens)}\\n\")\n",
    "\n",
    "# Show examples where lemmatization made a difference\n",
    "different_lemmas = [t for t in lemmatized_tokens if t['original'].lower() != t['lemma'].lower()]\n",
    "\n",
    "print(\"Examples where lemmatization changed the word:\")\n",
    "lemma_df = pd.DataFrame(different_lemmas[:20])\n",
    "print(lemma_df.to_string(index=False))\n",
    "\n",
    "# Most common lemmas (excluding stop words)\n",
    "non_stop_lemmas = [t['lemma'].lower() for t in lemmatized_tokens if not t['is_stop']]\n",
    "common_lemmas = Counter(non_stop_lemmas).most_common(10)\n",
    "\n",
    "print(f\"\\nMost common lemmas (excluding stop words):\")\n",
    "for lemma, count in common_lemmas:\n",
    "    print(f\"{lemma}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Entity Masking\n",
    "\n",
    "Named Entity Recognition (NER) identifies and masks named entities in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_masking(text, nlp_model, mask_char=\"[MASK]\"):\n",
    "    \"\"\"\n",
    "    Identify named entities and create masked version of text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        nlp_model: Loaded spaCy model\n",
    "        mask_char (str): Character/string to use for masking\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (entities_info, masked_text)\n",
    "    \"\"\"\n",
    "    doc = nlp_model(text)\n",
    "    entities = []\n",
    "    \n",
    "    # Collect entity information\n",
    "    for ent in doc.ents:\n",
    "        entities.append({\n",
    "            'text': ent.text,\n",
    "            'label': ent.label_,\n",
    "            'description': spacy.explain(ent.label_),\n",
    "            'start': ent.start_char,\n",
    "            'end': ent.end_char\n",
    "        })\n",
    "    \n",
    "    # Create masked text\n",
    "    masked_text = text\n",
    "    # Sort entities by start position in reverse order to maintain indices\n",
    "    for ent in sorted(doc.ents, key=lambda x: x.start_char, reverse=True):\n",
    "        mask_replacement = f\"{mask_char}_{ent.label_}\"\n",
    "        masked_text = masked_text[:ent.start_char] + mask_replacement + masked_text[ent.end_char:]\n",
    "    \n",
    "    return entities, masked_text\n",
    "\n",
    "# Apply entity masking\n",
    "entities, masked_text = entity_masking(abstract, nlp)\n",
    "\n",
    "print(\"Entity Masking Results:\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"Found {len(entities)} named entities\\n\")\n",
    "\n",
    "# Display entities\n",
    "if entities:\n",
    "    entities_df = pd.DataFrame(entities)\n",
    "    print(\"Named Entities Found:\")\n",
    "    print(entities_df[['text', 'label', 'description']].to_string(index=False))\n",
    "    \n",
    "    # Entity label distribution\n",
    "    entity_counts = Counter([ent['label'] for ent in entities])\n",
    "    print(f\"\\nEntity Label Distribution:\")\n",
    "    for label, count in entity_counts.items():\n",
    "        print(f\"{label}: {count}\")\n",
    "else:\n",
    "    print(\"No named entities found.\")\n",
    "\n",
    "print(f\"\\nMasked Text:\")\n",
    "print(\"-\" * 40)\n",
    "print(masked_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. POS Tagger\n",
    "\n",
    "Part-of-Speech tagging identifies the grammatical role of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Apply Part-of-Speech tagging to text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        nlp_model: Loaded spaCy model\n",
    "    \n",
    "    Returns:\n",
    "        list: List of tokens with POS information\n",
    "    \"\"\"\n",
    "    doc = nlp_model(text)\n",
    "    pos_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_space:\n",
    "            pos_tokens.append({\n",
    "                'text': token.text,\n",
    "                'pos': token.pos_,\n",
    "                'tag': token.tag_,\n",
    "                'pos_description': spacy.explain(token.pos_),\n",
    "                'tag_description': spacy.explain(token.tag_)\n",
    "            })\n",
    "    \n",
    "    return pos_tokens\n",
    "\n",
    "# Apply POS tagging\n",
    "pos_tokens = pos_tagging(abstract, nlp)\n",
    "\n",
    "print(\"POS Tagging Results:\")\n",
    "print(\"=\" * 22)\n",
    "print(f\"Total tokens: {len(pos_tokens)}\\n\")\n",
    "\n",
    "# Show first 15 tokens with POS tags\n",
    "pos_df = pd.DataFrame(pos_tokens[:15])\n",
    "print(\"First 15 tokens with POS tags:\")\n",
    "print(pos_df[['text', 'pos', 'pos_description']].to_string(index=False))\n",
    "\n",
    "# POS tag distribution\n",
    "pos_counts = Counter([token['pos'] for token in pos_tokens])\n",
    "print(f\"\\nPOS Tag Distribution:\")\n",
    "for pos, count in pos_counts.most_common():\n",
    "    description = spacy.explain(pos) or pos\n",
    "    print(f\"{pos} ({description}): {count}\")\n",
    "\n",
    "# Visualize POS distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "pos_labels, pos_counts_values = zip(*pos_counts.most_common())\n",
    "plt.bar(pos_labels, pos_counts_values)\n",
    "plt.title('Part-of-Speech Tag Distribution')\n",
    "plt.xlabel('POS Tags')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Phrase Chunking\n",
    "\n",
    "Phrase chunking groups related tokens into meaningful phrases using dependency parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_chunking(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Extract noun phrases and other meaningful chunks from text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        nlp_model: Loaded spaCy model\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing different types of phrases\n",
    "    \"\"\"\n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    # Extract noun phrases\n",
    "    noun_phrases = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        noun_phrases.append({\n",
    "            'text': chunk.text,\n",
    "            'root': chunk.root.text,\n",
    "            'root_dep': chunk.root.dep_,\n",
    "            'root_head': chunk.root.head.text\n",
    "        })\n",
    "    \n",
    "    # Extract verb phrases (simplified - tokens with verb POS and their objects)\n",
    "    verb_phrases = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            # Get verb and its direct objects/complements\n",
    "            phrase_tokens = [token.text]\n",
    "            for child in token.children:\n",
    "                if child.dep_ in ['dobj', 'iobj', 'attr', 'prep']:\n",
    "                    phrase_tokens.append(child.text)\n",
    "            \n",
    "            if len(phrase_tokens) > 1:\n",
    "                verb_phrases.append({\n",
    "                    'text': ' '.join(phrase_tokens),\n",
    "                    'root_verb': token.text,\n",
    "                    'dependencies': [child.dep_ for child in token.children]\n",
    "                })\n",
    "    \n",
    "    return {\n",
    "        'noun_phrases': noun_phrases,\n",
    "        'verb_phrases': verb_phrases\n",
    "    }\n",
    "\n",
    "# Apply phrase chunking\n",
    "phrases = phrase_chunking(abstract, nlp)\n",
    "\n",
    "print(\"Phrase Chunking Results:\")\n",
    "print(\"=\" * 26)\n",
    "\n",
    "# Display noun phrases\n",
    "print(f\"Found {len(phrases['noun_phrases'])} noun phrases:\")\n",
    "if phrases['noun_phrases']:\n",
    "    np_df = pd.DataFrame(phrases['noun_phrases'])\n",
    "    print(np_df[['text', 'root', 'root_dep']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nFound {len(phrases['verb_phrases'])} verb phrases:\")\n",
    "if phrases['verb_phrases']:\n",
    "    vp_df = pd.DataFrame(phrases['verb_phrases'])\n",
    "    print(vp_df[['text', 'root_verb']].to_string(index=False))\n",
    "\n",
    "# Most common noun phrase roots\n",
    "np_roots = [np['root'] for np in phrases['noun_phrases']]\n",
    "common_np_roots = Counter(np_roots).most_common(5)\n",
    "\n",
    "print(f\"\\nMost common noun phrase roots:\")\n",
    "for root, count in common_np_roots:\n",
    "    print(f\"{root}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Syntactic Parser\n",
    "\n",
    "Syntactic parsing analyzes the grammatical structure and dependencies between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntactic_parsing(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Analyze syntactic dependencies and grammatical structure.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        nlp_model: Loaded spaCy model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (dependency_info, sentence_structures)\n",
    "    \"\"\"\n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    # Extract dependency information\n",
    "    dependencies = []\n",
    "    for token in doc:\n",
    "        if not token.is_space:\n",
    "            dependencies.append({\n",
    "                'text': token.text,\n",
    "                'dep': token.dep_,\n",
    "                'dep_description': spacy.explain(token.dep_),\n",
    "                'head': token.head.text,\n",
    "                'pos': token.pos_,\n",
    "                'children': [child.text for child in token.children]\n",
    "            })\n",
    "    \n",
    "    # Analyze sentence structure for each sentence\n",
    "    sentence_structures = []\n",
    "    for sent in doc.sents:\n",
    "        # Find root of sentence\n",
    "        root = [token for token in sent if token.dep_ == 'ROOT'][0]\n",
    "        \n",
    "        structure = {\n",
    "            'sentence': sent.text.strip(),\n",
    "            'root': root.text,\n",
    "            'root_pos': root.pos_,\n",
    "            'subjects': [token.text for token in sent if token.dep_ in ['nsubj', 'nsubjpass']],\n",
    "            'objects': [token.text for token in sent if token.dep_ in ['dobj', 'iobj', 'pobj']],\n",
    "            'modifiers': [token.text for token in sent if token.dep_ in ['amod', 'advmod', 'nummod']]\n",
    "        }\n",
    "        sentence_structures.append(structure)\n",
    "    \n",
    "    return dependencies, sentence_structures\n",
    "\n",
    "# Apply syntactic parsing\n",
    "dependencies, sentence_structures = syntactic_parsing(abstract, nlp)\n",
    "\n",
    "print(\"Syntactic Parsing Results:\")\n",
    "print(\"=\" * 28)\n",
    "\n",
    "# Show dependency information for first 15 tokens\n",
    "print(\"Dependency relationships (first 15 tokens):\")\n",
    "dep_df = pd.DataFrame(dependencies[:15])\n",
    "print(dep_df[['text', 'dep', 'head', 'pos']].to_string(index=False))\n",
    "\n",
    "# Dependency label distribution\n",
    "dep_counts = Counter([dep['dep'] for dep in dependencies])\n",
    "print(f\"\\nMost common dependency labels:\")\n",
    "for dep_label, count in dep_counts.most_common(10):\n",
    "    description = spacy.explain(dep_label) or dep_label\n",
    "    print(f\"{dep_label} ({description}): {count}\")\n",
    "\n",
    "# Sentence structure analysis\n",
    "print(f\"\\nSentence Structure Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "for i, struct in enumerate(sentence_structures[:3], 1):\n",
    "    print(f\"\\nSentence {i}: {struct['sentence'][:80]}...\")\n",
    "    print(f\"Root: {struct['root']} ({struct['root_pos']})\")\n",
    "    print(f\"Subjects: {', '.join(struct['subjects']) if struct['subjects'] else 'None'}\")\n",
    "    print(f\"Objects: {', '.join(struct['objects']) if struct['objects'] else 'None'}\")\n",
    "    print(f\"Modifiers: {', '.join(struct['modifiers'][:5]) if struct['modifiers'] else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Analysis Summary\n",
    "\n",
    "Let's create a comprehensive summary of all the NLP processing results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_analysis_summary():\n",
    "    \"\"\"\n",
    "    Create a comprehensive summary of all NLP analysis results.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMPREHENSIVE NLP ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\n1. TEXT STATISTICS:\")\n",
    "    print(f\"   • Original text length: {len(abstract)} characters\")\n",
    "    print(f\"   • Number of sentences: {len(sentences)}\")\n",
    "    print(f\"   • Total tokens: {len(tokens)}\")\n",
    "    print(f\"   • Alphabetic tokens: {sum(1 for t in tokens if t['is_alpha'])}\")\n",
    "    print(f\"   • Stop words: {sum(1 for t in tokens if t['is_stop'])}\")\n",
    "    \n",
    "    print(f\"\\n2. MORPHOLOGICAL ANALYSIS:\")\n",
    "    print(f\"   • Stemmed tokens: {len(stemmed_tokens)}\")\n",
    "    print(f\"   • Lemmatized tokens: {len(lemmatized_tokens)}\")\n",
    "    print(f\"   • Tokens changed by lemmatization: {len([t for t in lemmatized_tokens if t['original'].lower() != t['lemma'].lower()])}\")\n",
    "    \n",
    "    print(f\"\\n3. NAMED ENTITY RECOGNITION:\")\n",
    "    print(f\"   • Named entities found: {len(entities)}\")\n",
    "    if entities:\n",
    "        entity_types = Counter([ent['label'] for ent in entities])\n",
    "        for ent_type, count in entity_types.items():\n",
    "            print(f\"   • {ent_type}: {count}\")\n",
    "    \n",
    "    print(f\"\\n4. SYNTACTIC ANALYSIS:\")\n",
    "    print(f\"   • Noun phrases: {len(phrases['noun_phrases'])}\")\n",
    "    print(f\"   • Verb phrases: {len(phrases['verb_phrases'])}\")\n",
    "    print(f\"   • Dependency relationships: {len(dependencies)}\")\n",
    "    \n",
    "    print(f\"\\n5. PART-OF-SPEECH DISTRIBUTION:\")\n",
    "    pos_counts = Counter([token['pos'] for token in pos_tokens])\n",
    "    for pos, count in pos_counts.most_common(5):\n",
    "        pos_desc = spacy.explain(pos) or pos\n",
    "        percentage = (count / len(pos_tokens)) * 100\n",
    "        print(f\"   • {pos} ({pos_desc}): {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n6. KEY FINDINGS:\")\n",
    "    # Most common meaningful words (excluding stop words)\n",
    "    meaningful_words = [t['lemma'].lower() for t in lemmatized_tokens \n",
    "                       if not t['is_stop'] and len(t['lemma']) > 2]\n",
    "    common_words = Counter(meaningful_words).most_common(5)\n",
    "    print(f\"   • Most frequent meaningful words: {', '.join([w[0] for w in common_words])}\")\n",
    "    \n",
    "    # Sentence complexity (average tokens per sentence)\n",
    "    avg_tokens_per_sentence = len([t for t in tokens if t['is_alpha']]) / len(sentences)\n",
    "    print(f\"   • Average tokens per sentence: {avg_tokens_per_sentence:.1f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Generate comprehensive summary\n",
    "comprehensive_analysis_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Results\n",
    "\n",
    "Let's create some visualizations to better understand the analysis results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Token type distribution\n",
    "token_types = ['Alphabetic', 'Punctuation', 'Stop Words', 'Other']\n",
    "token_counts = [\n",
    "    sum(1 for t in tokens if t['is_alpha']),\n",
    "    sum(1 for t in tokens if t['is_punct']),\n",
    "    sum(1 for t in tokens if t['is_stop']),\n",
    "    len(tokens) - sum(1 for t in tokens if t['is_alpha'] or t['is_punct'] or t['is_stop'])\n",
    "]\n",
    "\n",
    "axes[0, 0].pie(token_counts, labels=token_types, autopct='%1.1f%%', startangle=90)\n",
    "axes[0, 0].set_title('Token Type Distribution')\n",
    "\n",
    "# 2. Named entity distribution\n",
    "if entities:\n",
    "    entity_labels = [ent['label'] for ent in entities]\n",
    "    entity_counts = Counter(entity_labels)\n",
    "    \n",
    "    axes[0, 1].bar(entity_counts.keys(), entity_counts.values())\n",
    "    axes[0, 1].set_title('Named Entity Distribution')\n",
    "    axes[0, 1].set_xlabel('Entity Types')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    axes[0, 1].text(0.5, 0.5, 'No Named Entities Found', \n",
    "                   horizontalalignment='center', verticalalignment='center',\n",
    "                   transform=axes[0, 1].transAxes)\n",
    "    axes[0, 1].set_title('Named Entity Distribution')\n",
    "\n",
    "# 3. Sentence length distribution\n",
    "sentence_lengths = [len(sent.split()) for sent in sentences]\n",
    "axes[1, 0].hist(sentence_lengths, bins=len(set(sentence_lengths)), alpha=0.7)\n",
    "axes[1, 0].set_title('Sentence Length Distribution')\n",
    "axes[1, 0].set_xlabel('Number of Words')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 4. Top dependency relationships\n",
    "dep_counts = Counter([dep['dep'] for dep in dependencies])\n",
    "top_deps = dict(dep_counts.most_common(8))\n",
    "\n",
    "axes[1, 1].bar(top_deps.keys(), top_deps.values())\n",
    "axes[1, 1].set_title('Top Dependency Relationships')\n",
    "axes[1, 1].set_xlabel('Dependency Labels')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualizations created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}