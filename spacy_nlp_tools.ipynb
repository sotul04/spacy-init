{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy NLP Tools Implementation\n",
    "\n",
    "This notebook demonstrates the implementation of 8 essential NLP processing tools using SpaCy:\n",
    "\n",
    "1. **Sentence Splitter** - Splits text into individual sentences\n",
    "2. **Tokenization** - Breaks text into individual tokens (words, punctuation)\n",
    "3. **Stemming** - Reduces words to their root form\n",
    "4. **Lemmatization** - Reduces words to their canonical/dictionary form\n",
    "5. **Entity Masking** - Identifies and masks named entities\n",
    "6. **POS Tagger** - Identifies parts of speech for each token\n",
    "7. **Phrase Chunking** - Groups tokens into meaningful phrases\n",
    "8. **Syntactic Parser** - Analyzes grammatical structure and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy version: 3.8.7\n",
      "Model loaded: core_web_sm v3.8.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from typing import List, Dict\n",
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(f\"SpaCy version: {spacy.__version__}\")\n",
    "print(f\"Model loaded: {nlp.meta['name']} v{nlp.meta['version']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Research Paper Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Paper Abstract:\n",
      "==================================================\n",
      "Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning.\n",
      "RNNs, however, are inherently sequential models that do not allow parallelization of their computations.\n",
      "Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations \n",
      "with a multi-head attention mechanism. In this paper, we propose the SepFormer, a novel RNN-free\n",
      "Transformer-based neural network for speech separation. The Sep-Former learns short and long-term\n",
      "dependencies with a multi-scale approach that employs transformers. The proposed model achieves\n",
      "state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It reaches an SI-SNRi\n",
      "of 22.3 dB on WSJ0-2mix and an SI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the parallelization\n",
      "advantages of Transformers and achieves a competitive performance even when downsampling the encoded\n",
      "representation by a factor of 8. It is thus significantly faster and it is less memory-demanding\n",
      "than the latest speech separation systems with comparable performance.\n",
      "\n",
      "Text length: 1078 characters\n"
     ]
    }
   ],
   "source": [
    "# Research paper abstract - \"Attention Is All You Need In Speech Separation\" by Subakan et al.\n",
    "abstract = \"\"\"\n",
    "Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning.\n",
    "RNNs, however, are inherently sequential models that do not allow parallelization of their computations.\n",
    "Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations \n",
    "with a multi-head attention mechanism. In this paper, we propose the SepFormer, a novel RNN-free\n",
    "Transformer-based neural network for speech separation. The Sep-Former learns short and long-term\n",
    "dependencies with a multi-scale approach that employs transformers. The proposed model achieves\n",
    "state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It reaches an SI-SNRi\n",
    "of 22.3 dB on WSJ0-2mix and an SI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the parallelization\n",
    "advantages of Transformers and achieves a competitive performance even when downsampling the encoded\n",
    "representation by a factor of 8. It is thus significantly faster and it is less memory-demanding\n",
    "than the latest speech separation systems with comparable performance.\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"Research Paper Abstract:\")\n",
    "print(\"=\" * 50)\n",
    "print(abstract)\n",
    "print(\"\\nText length:\", len(abstract), \"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sentence Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Splitter Results:\n",
      "==============================\n",
      "Number of sentences: 9\n",
      "\n",
      "Sentence 1: Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning.\n",
      "\n",
      "Sentence 2: RNNs, however, are inherently sequential models that do not allow parallelization of their computations.\n",
      "\n",
      "Sentence 3: Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations \n",
      "with a multi-head attention mechanism.\n",
      "\n",
      "Sentence 4: In this paper, we propose the SepFormer, a novel RNN-free\n",
      "Transformer-based neural network for speech separation.\n",
      "\n",
      "Sentence 5: The Sep-Former learns short and long-term\n",
      "dependencies with a multi-scale approach that employs transformers.\n",
      "\n",
      "Sentence 6: The proposed model achieves\n",
      "state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets.\n",
      "\n",
      "Sentence 7: It reaches an SI-SNRi\n",
      "of 22.3 dB on WSJ0-2mix and an SI-SNRi of 19.5 dB on WSJ0-3mix.\n",
      "\n",
      "Sentence 8: The SepFormer inherits the parallelization\n",
      "advantages of Transformers and achieves a competitive performance even when downsampling the encoded\n",
      "representation by a factor of 8.\n",
      "\n",
      "Sentence 9: It is thus significantly faster and it is less memory-demanding\n",
      "than the latest speech separation systems with comparable performance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sentence_splitter(text: str, nlp_model: Language) -> List[str]:\n",
    "    doc = nlp_model(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "sentences = sentence_splitter(abstract, nlp)\n",
    "\n",
    "print(\"Sentence Splitter Results:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Number of sentences: {len(sentences)}\\n\")\n",
    "\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization Results:\n",
      "=========================\n",
      "Total tokens: 209\n",
      "\n",
      "First 40 tokens:\n",
      "           text  is_alpha  is_punct  is_space  is_stop shape\n",
      "      Recurrent      True     False     False    False Xxxxx\n",
      "         Neural      True     False     False    False Xxxxx\n",
      "       Networks      True     False     False    False Xxxxx\n",
      "              (     False      True     False    False     (\n",
      "           RNNs      True     False     False    False  XXXx\n",
      "              )     False      True     False    False     )\n",
      "           have      True     False     False     True  xxxx\n",
      "           long      True     False     False    False  xxxx\n",
      "           been      True     False     False     True  xxxx\n",
      "            the      True     False     False     True   xxx\n",
      "       dominant      True     False     False    False  xxxx\n",
      "   architecture      True     False     False    False  xxxx\n",
      "             in      True     False     False     True    xx\n",
      "       sequence      True     False     False    False  xxxx\n",
      "              -     False      True     False    False     -\n",
      "             to      True     False     False     True    xx\n",
      "              -     False      True     False    False     -\n",
      "       sequence      True     False     False    False  xxxx\n",
      "       learning      True     False     False    False  xxxx\n",
      "              .     False      True     False    False     .\n",
      "             \\n     False     False      True    False    \\n\n",
      "           RNNs      True     False     False    False  XXXx\n",
      "              ,     False      True     False    False     ,\n",
      "        however      True     False     False     True  xxxx\n",
      "              ,     False      True     False    False     ,\n",
      "            are      True     False     False     True   xxx\n",
      "     inherently      True     False     False    False  xxxx\n",
      "     sequential      True     False     False    False  xxxx\n",
      "         models      True     False     False    False  xxxx\n",
      "           that      True     False     False     True  xxxx\n",
      "             do      True     False     False     True    xx\n",
      "            not      True     False     False     True   xxx\n",
      "          allow      True     False     False    False  xxxx\n",
      "parallelization      True     False     False    False  xxxx\n",
      "             of      True     False     False     True    xx\n",
      "          their      True     False     False     True  xxxx\n",
      "   computations      True     False     False    False  xxxx\n",
      "              .     False      True     False    False     .\n",
      "             \\n     False     False      True    False    \\n\n",
      "   Transformers      True     False     False    False Xxxxx\n",
      "\n",
      "Token Statistics:\n",
      "Alphabetic tokens: 155\n",
      "Punctuation tokens: 35\n",
      "Stop words: 64\n"
     ]
    }
   ],
   "source": [
    "def tokenization(text: str, nlp_model: Language):\n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        tokens.append({\n",
    "            'text': token.text,\n",
    "            'is_alpha': token.is_alpha,\n",
    "            'is_punct': token.is_punct,\n",
    "            'is_space': token.is_space,\n",
    "            'is_stop': token.is_stop,\n",
    "            'shape': token.shape_\n",
    "        })\n",
    "    \n",
    "    return tokens, doc\n",
    "\n",
    "tokens, doc = tokenization(abstract, nlp)\n",
    "\n",
    "print(\"Tokenization Results:\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"Total tokens: {len(tokens)}\\n\")\n",
    "\n",
    "token_df = pd.DataFrame(tokens[:40])\n",
    "print(\"First 40 tokens:\")\n",
    "print(token_df.to_string(index=False))\n",
    "\n",
    "alpha_tokens = sum(1 for t in tokens if t['is_alpha'])\n",
    "punct_tokens = sum(1 for t in tokens if t['is_punct'])\n",
    "stop_tokens = sum(1 for t in tokens if t['is_stop'])\n",
    "\n",
    "print(f\"\\nToken Statistics:\")\n",
    "print(f\"Alphabetic tokens: {alpha_tokens}\")\n",
    "print(f\"Punctuation tokens: {punct_tokens}\")\n",
    "print(f\"Stop words: {stop_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming Results:\n",
      "====================\n",
      "Total stemmed tokens: 91\n",
      "\n",
      "Examples where stemming changed the word:\n",
      "       original       stemmed   pos\n",
      "       Networks       network PROPN\n",
      "           RNNs           rnn PROPN\n",
      "       learning         learn  NOUN\n",
      "           RNNs           rnn  NOUN\n",
      "         models         model  NOUN\n",
      "parallelization parallelizate  NOUN\n",
      "   computations   computation  NOUN\n",
      "   Transformers   transformer  NOUN\n",
      "       emerging         emerg  VERB\n",
      "           RNNs           rnn  NOUN\n",
      "      replacing        replac  VERB\n",
      "   computations   computation  NOUN\n",
      "          paper           pap  NOUN\n",
      "      SepFormer       sepform PROPN\n",
      "    Transformer     transform PROPN\n"
     ]
    }
   ],
   "source": [
    "def simple_stemmer(word: str) -> str:\n",
    "    word = word.lower()\n",
    "\n",
    "    # Ordered suffixes (longer first to avoid conflicts)\n",
    "    suffixes = [\n",
    "        (\"ational\", \"ate\"),\n",
    "        (\"tional\", \"tion\"),\n",
    "        (\"iveness\", \"ive\"),\n",
    "        (\"fulness\", \"ful\"),\n",
    "        (\"ousness\", \"ous\"),\n",
    "        (\"biliti\", \"ble\"),\n",
    "        (\"lessli\", \"less\"),\n",
    "        (\"entli\", \"ent\"),\n",
    "        (\"ation\", \"ate\"),\n",
    "        (\"izer\", \"ize\"),\n",
    "        (\"ing\", \"\"),\n",
    "        (\"edly\", \"\"),\n",
    "        (\"edly\", \"e\"),\n",
    "        (\"edly\", \"\"),\n",
    "        (\"ed\", \"\"),\n",
    "        (\"ies\", \"y\"),\n",
    "        (\"ied\", \"y\"),\n",
    "        (\"s\", \"\"),\n",
    "        (\"es\", \"\"),\n",
    "        (\"er\", \"\"),\n",
    "        (\"est\", \"\"),\n",
    "        (\"ness\", \"\"),\n",
    "        (\"ment\", \"\"),\n",
    "    ]\n",
    "\n",
    "    for suffix, replacement in suffixes:\n",
    "        if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
    "            stem = word[:-len(suffix)] + replacement\n",
    "\n",
    "            # Fix double consonants after removing \"ing\" or \"ed\"\n",
    "            if suffix in (\"ing\", \"ed\") and len(stem) > 2 and stem[-1] == stem[-2]:\n",
    "                stem = stem[:-1]\n",
    "\n",
    "            return stem\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "def stemming(text: str, nlp_model: Language) -> List[Dict[str, str]]:\n",
    "    doc = nlp_model(text)\n",
    "    stemmed_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_alpha and not token.is_stop:\n",
    "            stemmed = simple_stemmer(token.text)\n",
    "            stemmed_tokens.append({\n",
    "                \"original\": token.text,\n",
    "                \"stemmed\": stemmed,\n",
    "                \"pos\": token.pos_\n",
    "            })\n",
    "    \n",
    "    return stemmed_tokens\n",
    "\n",
    "stemmed_tokens = stemming(abstract, nlp)\n",
    "\n",
    "print(\"Stemming Results:\")\n",
    "print(\"=\" * 20)\n",
    "print(f\"Total stemmed tokens: {len(stemmed_tokens)}\\n\")\n",
    "\n",
    "different_stems = [t for t in stemmed_tokens if t['original'].lower() != t['stemmed']]\n",
    "\n",
    "print(\"Examples where stemming changed the word:\")\n",
    "stem_df = pd.DataFrame(different_stems[:15])\n",
    "if not stem_df.empty:\n",
    "    print(stem_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No significant stemming changes found with simple rules.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Results:\n",
      "=========================\n",
      "Total lemmatized tokens: 155\n",
      "\n",
      "Examples where lemmatization changed the word:\n",
      "    original       lemma  pos  is_stop\n",
      "        been          be  AUX     True\n",
      "        RNNs         rnn NOUN    False\n",
      "         are          be  AUX     True\n",
      "      models       model NOUN    False\n",
      "computations computation NOUN    False\n",
      "Transformers transformer NOUN    False\n",
      "         are          be  AUX     True\n",
      "    emerging      emerge VERB    False\n",
      "        RNNs         rnn NOUN    False\n",
      "   replacing     replace VERB    False\n",
      "computations computation NOUN    False\n",
      "       based        base VERB    False\n",
      "      learns       learn VERB    False\n",
      "dependencies  dependency NOUN    False\n",
      "     employs      employ VERB    False\n",
      "transformers transformer NOUN    False\n",
      "    proposed     propose VERB    False\n",
      "    achieves     achieve VERB    False\n",
      "    datasets     dataset NOUN    False\n",
      "     reaches       reach VERB    False\n",
      "\n",
      "Most common lemmas (excluding stop words):\n",
      "rnn: 3\n",
      "transformer: 3\n",
      "performance: 3\n",
      "recurrent: 2\n",
      "neural: 2\n",
      "long: 2\n",
      "sequence: 2\n",
      "model: 2\n",
      "parallelization: 2\n",
      "computation: 2\n"
     ]
    }
   ],
   "source": [
    "def lemmatization(text, nlp_model):\n",
    "    doc = nlp_model(text)\n",
    "    lemmatized_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            lemmatized_tokens.append({\n",
    "                'original': token.text,\n",
    "                'lemma': token.lemma_,\n",
    "                'pos': token.pos_,\n",
    "                'is_stop': token.is_stop\n",
    "            })\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "lemmatized_tokens = lemmatization(abstract, nlp)\n",
    "\n",
    "print(\"Lemmatization Results:\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"Total lemmatized tokens: {len(lemmatized_tokens)}\\n\")\n",
    "\n",
    "different_lemmas = [t for t in lemmatized_tokens if t['original'].lower() != t['lemma'].lower()]\n",
    "\n",
    "print(\"Examples where lemmatization changed the word:\")\n",
    "lemma_df = pd.DataFrame(different_lemmas[:20])\n",
    "print(lemma_df.to_string(index=False))\n",
    "\n",
    "non_stop_lemmas = [t['lemma'].lower() for t in lemmatized_tokens if not t['is_stop']]\n",
    "common_lemmas = Counter(non_stop_lemmas).most_common(10)\n",
    "\n",
    "print(f\"\\nMost common lemmas (excluding stop words):\")\n",
    "for lemma, count in common_lemmas:\n",
    "    print(f\"{lemma}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Entity Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Masking Results:\n",
      "=========================\n",
      "Found 9 named entities\n",
      "\n",
      "Named Entities Found:\n",
      "                     text    label                                          description\n",
      "Recurrent Neural Networks      ORG              Companies, agencies, institutions, etc.\n",
      "                SepFormer      ORG              Companies, agencies, institutions, etc.\n",
      "              WSJ0-2/3mix    EVENT Named hurricanes, battles, wars, sports events, etc.\n",
      "                     22.3 CARDINAL         Numerals that do not fall under another type\n",
      "                WSJ0-2mix     DATE                Absolute or relative dates or periods\n",
      "                     19.5 CARDINAL         Numerals that do not fall under another type\n",
      "                WSJ0-3mix     DATE                Absolute or relative dates or periods\n",
      "                SepFormer      ORG              Companies, agencies, institutions, etc.\n",
      "                        8 CARDINAL         Numerals that do not fall under another type\n",
      "\n",
      "Entity Label Distribution:\n",
      "ORG: 3\n",
      "EVENT: 1\n",
      "CARDINAL: 3\n",
      "DATE: 2\n",
      "\n",
      "Masked Text:\n",
      "----------------------------------------\n",
      "[MASK]_ORG (RNNs) have long been the dominant architecture in sequence-to-sequence learning.\n",
      "RNNs, however, are inherently sequential models that do not allow parallelization of their computations.\n",
      "Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations \n",
      "with a multi-head attention mechanism. In this paper, we propose the [MASK]_ORG, a novel RNN-free\n",
      "Transformer-based neural network for speech separation. The Sep-Former learns short and long-term\n",
      "dependencies with a multi-scale approach that employs transformers. The proposed model achieves\n",
      "state-of-the-art (SOTA) performance on the standard [MASK]_EVENT datasets. It reaches an SI-SNRi\n",
      "of [MASK]_CARDINAL dB on [MASK]_DATE and an SI-SNRi of [MASK]_CARDINAL dB on [MASK]_DATE. The [MASK]_ORG inherits the parallelization\n",
      "advantages of Transformers and achieves a competitive performance even when downsampling the encoded\n",
      "representation by a factor of [MASK]_CARDINAL. It is thus significantly faster and it is less memory-demanding\n",
      "than the latest speech separation systems with comparable performance.\n",
      "\n",
      "\n",
      "Original Text:\n",
      "----------------------------------------\n",
      "Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning.\n",
      "RNNs, however, are inherently sequential models that do not allow parallelization of their computations.\n",
      "Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations \n",
      "with a multi-head attention mechanism. In this paper, we propose the SepFormer, a novel RNN-free\n",
      "Transformer-based neural network for speech separation. The Sep-Former learns short and long-term\n",
      "dependencies with a multi-scale approach that employs transformers. The proposed model achieves\n",
      "state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It reaches an SI-SNRi\n",
      "of 22.3 dB on WSJ0-2mix and an SI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the parallelization\n",
      "advantages of Transformers and achieves a competitive performance even when downsampling the encoded\n",
      "representation by a factor of 8. It is thus significantly faster and it is less memory-demanding\n",
      "than the latest speech separation systems with comparable performance.\n"
     ]
    }
   ],
   "source": [
    "def entity_masking(text, nlp_model, mask_char=\"[MASK]\"):\n",
    "    doc = nlp_model(text)\n",
    "    entities = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        entities.append({\n",
    "            'text': ent.text,\n",
    "            'label': ent.label_,\n",
    "            'description': spacy.explain(ent.label_),\n",
    "            'start': ent.start_char,\n",
    "            'end': ent.end_char\n",
    "        })\n",
    "    \n",
    "    masked_text = text\n",
    "    for ent in sorted(doc.ents, key=lambda x: x.start_char, reverse=True):\n",
    "        mask_replacement = f\"{mask_char}_{ent.label_}\"\n",
    "        masked_text = masked_text[:ent.start_char] + mask_replacement + masked_text[ent.end_char:]\n",
    "    \n",
    "    return entities, masked_text\n",
    "\n",
    "entities, masked_text = entity_masking(abstract, nlp)\n",
    "\n",
    "print(\"Entity Masking Results:\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"Found {len(entities)} named entities\\n\")\n",
    "\n",
    "if entities:\n",
    "    entities_df = pd.DataFrame(entities)\n",
    "    print(\"Named Entities Found:\")\n",
    "    print(entities_df[['text', 'label', 'description']].to_string(index=False))\n",
    "    \n",
    "    entity_counts = Counter([ent['label'] for   ent in entities])\n",
    "    print(f\"\\nEntity Label Distribution:\")\n",
    "    for label, count in entity_counts.items():\n",
    "        print(f\"{label}: {count}\")\n",
    "else:\n",
    "    print(\"No named entities found.\")\n",
    "\n",
    "print(f\"\\nMasked Text:\")\n",
    "print(\"-\" * 40)\n",
    "print(masked_text)\n",
    "print(\"\\n\\nOriginal Text:\")\n",
    "print(\"-\" * 40)\n",
    "print(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging Results:\n",
      "======================\n",
      "Total tokens: 199\n",
      "\n",
      "First 15 tokens with POS tags:\n",
      "        text   pos pos_description\n",
      "   Recurrent PROPN     proper noun\n",
      "      Neural PROPN     proper noun\n",
      "    Networks PROPN     proper noun\n",
      "           ( PUNCT     punctuation\n",
      "        RNNs PROPN     proper noun\n",
      "           ) PUNCT     punctuation\n",
      "        have   AUX       auxiliary\n",
      "        long   ADV          adverb\n",
      "        been   AUX       auxiliary\n",
      "         the   DET      determiner\n",
      "    dominant   ADJ       adjective\n",
      "architecture  NOUN            noun\n",
      "          in   ADP      adposition\n",
      "    sequence  NOUN            noun\n",
      "           - PUNCT     punctuation\n",
      "\n",
      "POS Tag Distribution:\n",
      "NOUN (noun): 39\n",
      "PUNCT (punctuation): 33\n",
      "PROPN (proper noun): 22\n",
      "ADJ (adjective): 22\n",
      "ADP (adposition): 20\n",
      "DET (determiner): 19\n",
      "VERB (verb): 14\n",
      "AUX (auxiliary): 7\n",
      "ADV (adverb): 7\n",
      "PRON (pronoun): 7\n",
      "CCONJ (coordinating conjunction): 4\n",
      "NUM (numeral): 3\n",
      "PART (particle): 1\n",
      "SCONJ (subordinating conjunction): 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pos_tagging(text, nlp_model):\n",
    "    doc = nlp_model(text)\n",
    "    pos_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_space:\n",
    "            pos_tokens.append({\n",
    "                'text': token.text,\n",
    "                'pos': token.pos_,\n",
    "                'tag': token.tag_,\n",
    "                'pos_description': spacy.explain(token.pos_),\n",
    "                'tag_description': spacy.explain(token.tag_)\n",
    "            })\n",
    "    \n",
    "    return pos_tokens\n",
    "\n",
    "pos_tokens = pos_tagging(abstract, nlp)\n",
    "\n",
    "print(\"POS Tagging Results:\")\n",
    "print(\"=\" * 22)\n",
    "print(f\"Total tokens: {len(pos_tokens)}\\n\")\n",
    "\n",
    "pos_df = pd.DataFrame(pos_tokens[:15])\n",
    "print(\"First 15 tokens with POS tags:\")\n",
    "print(pos_df[['text', 'pos', 'pos_description']].to_string(index=False))\n",
    "\n",
    "pos_counts = Counter([token['pos'] for token in pos_tokens])\n",
    "print(f\"\\nPOS Tag Distribution:\")\n",
    "for pos, count in pos_counts.most_common():\n",
    "    description = spacy.explain(pos) or pos\n",
    "    print(f\"{pos} ({description}): {count}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "pos_labels, pos_counts_values = zip(*pos_counts.most_common())\n",
    "plt.bar(pos_labels, pos_counts_values)\n",
    "plt.title('Part-of-Speech Tag Distribution')\n",
    "plt.xlabel('POS Tags')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Phrase Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase Chunking Results:\n",
      "==========================\n",
      "Found 44 noun phrases:\n",
      "                                              text            root root_dep\n",
      "                         Recurrent Neural Networks        Networks    nsubj\n",
      "                                              RNNs            RNNs    appos\n",
      "                         the dominant architecture    architecture     attr\n",
      "                                          sequence        sequence     pobj\n",
      "                                              RNNs            RNNs    nsubj\n",
      "                                 sequential models          models     attr\n",
      "                                              that            that    nsubj\n",
      "                                   parallelization parallelization     dobj\n",
      "                                their computations    computations     pobj\n",
      "                                      Transformers    Transformers    nsubj\n",
      "                             a natural alternative     alternative     pobj\n",
      "                                     standard RNNs            RNNs     pobj\n",
      "                            recurrent computations    computations     dobj\n",
      "                  a multi-head attention mechanism       mechanism     pobj\n",
      "                                        this paper           paper     pobj\n",
      "                                                we              we    nsubj\n",
      "                                     the SepFormer       SepFormer     dobj\n",
      "a novel RNN-free\\nTransformer-based neural network         network    appos\n",
      "                                 speech separation      separation     pobj\n",
      "                                    The Sep-Former          Former    nsubj\n",
      "                 short and long-term\\ndependencies    dependencies     dobj\n",
      "                            a multi-scale approach        approach     pobj\n",
      "                                              that            that    nsubj\n",
      "                                      transformers    transformers     dobj\n",
      "                                The proposed model           model    nsubj\n",
      "                                           the-art             art     pobj\n",
      "                 the standard WSJ0-2/3mix datasets        datasets     pobj\n",
      "                                                It              It    nsubj\n",
      "                                        an SI-SNRi            SNRi     dobj\n",
      "                                                dB              dB     dobj\n",
      "                                         WSJ0-2mix            2mix     pobj\n",
      "                                        an SI-SNRi            SNRi     conj\n",
      "                                                dB              dB     dobj\n",
      "                                         WSJ0-3mix            3mix     pobj\n",
      "                                     The SepFormer       SepFormer    nsubj\n",
      "                   the parallelization\\nadvantages      advantages     dobj\n",
      "                                      Transformers    Transformers     pobj\n",
      "                         a competitive performance     performance     dobj\n",
      "                       the encoded\\nrepresentation  representation     dobj\n",
      "                                          a factor          factor     pobj\n",
      "                                                It              It    nsubj\n",
      "                                                it              it    nsubj\n",
      "              the latest speech separation systems         systems     pobj\n",
      "                            comparable performance     performance     pobj\n",
      "\n",
      "Found 12 verb phrases:\n",
      "                       text    root_verb\n",
      "      allow parallelization        allow\n",
      "                emerging as     emerging\n",
      "replacing computations with    replacing\n",
      "       propose In SepFormer      propose\n",
      "        learns dependencies       learns\n",
      "       employs transformers      employs\n",
      "       achieves performance     achieves\n",
      "   reaches SNRi dB on dB on      reaches\n",
      "        inherits advantages     inherits\n",
      "       achieves performance     achieves\n",
      "downsampling representation downsampling\n",
      "             demanding than    demanding\n",
      "\n",
      "Most common noun phrase roots:\n",
      "RNNs: 3\n",
      "that: 2\n",
      "computations: 2\n",
      "Transformers: 2\n",
      "SepFormer: 2\n"
     ]
    }
   ],
   "source": [
    "def phrase_chunking(text, nlp_model):\n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    noun_phrases = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        noun_phrases.append({\n",
    "            'text': chunk.text,\n",
    "            'root': chunk.root.text,\n",
    "            'root_dep': chunk.root.dep_,\n",
    "            'root_head': chunk.root.head.text\n",
    "        })\n",
    "    \n",
    "    verb_phrases = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            phrase_tokens = [token.text]\n",
    "            for child in token.children:\n",
    "                if child.dep_ in ['dobj', 'iobj', 'attr', 'prep']:\n",
    "                    phrase_tokens.append(child.text)\n",
    "            \n",
    "            if len(phrase_tokens) > 1:\n",
    "                verb_phrases.append({\n",
    "                    'text': ' '.join(phrase_tokens),\n",
    "                    'root_verb': token.text,\n",
    "                    'dependencies': [child.dep_ for child in token.children]\n",
    "                })\n",
    "    \n",
    "    return {\n",
    "        'noun_phrases': noun_phrases,\n",
    "        'verb_phrases': verb_phrases\n",
    "    }\n",
    "\n",
    "phrases = phrase_chunking(abstract, nlp)\n",
    "\n",
    "print(\"Phrase Chunking Results:\")\n",
    "print(\"=\" * 26)\n",
    "\n",
    "print(f\"Found {len(phrases['noun_phrases'])} noun phrases:\")\n",
    "if phrases['noun_phrases']:\n",
    "    np_df = pd.DataFrame(phrases['noun_phrases'])\n",
    "    print(np_df[['text', 'root', 'root_dep']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nFound {len(phrases['verb_phrases'])} verb phrases:\")\n",
    "if phrases['verb_phrases']:\n",
    "    vp_df = pd.DataFrame(phrases['verb_phrases'])\n",
    "    print(vp_df[['text', 'root_verb']].to_string(index=False))\n",
    "\n",
    "np_roots = [np['root'] for np in phrases['noun_phrases']]\n",
    "common_np_roots = Counter(np_roots).most_common(5)\n",
    "\n",
    "print(f\"\\nMost common noun phrase roots:\")\n",
    "for root, count in common_np_roots:\n",
    "    print(f\"{root}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Syntactic Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Parsing Results:\n",
      "============================\n",
      "Dependency relationships (first 15 tokens):\n",
      "        text      dep         head   pos\n",
      "   Recurrent compound     Networks PROPN\n",
      "      Neural compound     Networks PROPN\n",
      "    Networks    nsubj         been PROPN\n",
      "           (    punct     Networks PUNCT\n",
      "        RNNs    appos     Networks PROPN\n",
      "           )    punct     Networks PUNCT\n",
      "        have      aux         been   AUX\n",
      "        long   advmod         been   ADV\n",
      "        been     ROOT         been   AUX\n",
      "         the      det architecture   DET\n",
      "    dominant     amod architecture   ADJ\n",
      "architecture     attr         been  NOUN\n",
      "          in     prep architecture   ADP\n",
      "    sequence     nmod     learning  NOUN\n",
      "           -    punct     sequence PUNCT\n",
      "\n",
      "Most common dependency labels:\n",
      "punct (punctuation): 33\n",
      "amod (adjectival modifier): 22\n",
      "prep (prepositional modifier): 20\n",
      "pobj (object of preposition): 20\n",
      "det (determiner): 19\n",
      "compound (compound): 15\n",
      "nsubj (nominal subject): 12\n",
      "dobj (direct object): 12\n",
      "ROOT (root): 9\n",
      "advmod (adverbial modifier): 8\n",
      "\n",
      "Sentence Structure Analysis:\n",
      "------------------------------\n",
      "\n",
      "Sentence 1: Recurrent Neural Networks (RNNs) have long been the dominant architecture in seq...\n",
      "Root: been (AUX)\n",
      "Subjects: Networks\n",
      "Objects: sequence, learning\n",
      "Modifiers: long, dominant\n",
      "\n",
      "Sentence 2: RNNs, however, are inherently sequential models that do not allow parallelizatio...\n",
      "Root: are (AUX)\n",
      "Subjects: RNNs, that\n",
      "Objects: parallelization, computations\n",
      "Modifiers: however, inherently, sequential\n",
      "\n",
      "Sentence 3: Transformers are emerging as a natural alternative to standard RNNs, replacing r...\n",
      "Root: emerging (VERB)\n",
      "Subjects: Transformers\n",
      "Objects: alternative, RNNs, computations, mechanism\n",
      "Modifiers: natural, standard, recurrent, multi, -\n"
     ]
    }
   ],
   "source": [
    "def syntactic_parsing(text, nlp_model):\n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    dependencies = []\n",
    "    for token in doc:\n",
    "        if not token.is_space:\n",
    "            dependencies.append({\n",
    "                'text': token.text,\n",
    "                'dep': token.dep_,\n",
    "                'dep_description': spacy.explain(token.dep_),\n",
    "                'head': token.head.text,\n",
    "                'pos': token.pos_,\n",
    "                'children': [child.text for child in token.children]\n",
    "            })\n",
    "    \n",
    "    sentence_structures = []\n",
    "    for sent in doc.sents:\n",
    "        root = [token for token in sent if token.dep_ == 'ROOT'][0]\n",
    "        \n",
    "        structure = {\n",
    "            'sentence': sent.text.strip(),\n",
    "            'root': root.text,\n",
    "            'root_pos': root.pos_,\n",
    "            'subjects': [token.text for token in sent if token.dep_ in ['nsubj', 'nsubjpass']],\n",
    "            'objects': [token.text for token in sent if token.dep_ in ['dobj', 'iobj', 'pobj']],\n",
    "            'modifiers': [token.text for token in sent if token.dep_ in ['amod', 'advmod', 'nummod']]\n",
    "        }\n",
    "        sentence_structures.append(structure)\n",
    "    \n",
    "    return dependencies, sentence_structures\n",
    "\n",
    "dependencies, sentence_structures = syntactic_parsing(abstract, nlp)\n",
    "\n",
    "print(\"Syntactic Parsing Results:\")\n",
    "print(\"=\" * 28)\n",
    "\n",
    "print(\"Dependency relationships (first 15 tokens):\")\n",
    "dep_df = pd.DataFrame(dependencies[:15])\n",
    "print(dep_df[['text', 'dep', 'head', 'pos']].to_string(index=False))\n",
    "\n",
    "dep_counts = Counter([dep['dep'] for dep in dependencies])\n",
    "print(f\"\\nMost common dependency labels:\")\n",
    "for dep_label, count in dep_counts.most_common(10):\n",
    "    description = spacy.explain(dep_label) or dep_label\n",
    "    print(f\"{dep_label} ({description}): {count}\")\n",
    "\n",
    "print(f\"\\nSentence Structure Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "for i, struct in enumerate(sentence_structures[:3], 1):\n",
    "    print(f\"\\nSentence {i}: {struct['sentence'][:80]}...\")\n",
    "    print(f\"Root: {struct['root']} ({struct['root_pos']})\")\n",
    "    print(f\"Subjects: {', '.join(struct['subjects']) if struct['subjects'] else 'None'}\")\n",
    "    print(f\"Objects: {', '.join(struct['objects']) if struct['objects'] else 'None'}\")\n",
    "    print(f\"Modifiers: {', '.join(struct['modifiers'][:5]) if struct['modifiers'] else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract processing\n",
    "- Sentences\n",
    "- Entities\n",
    "- Masked Text\n",
    "- Noun Phrases\n",
    "- Triplets  insight berupa (Subject, Verb, Object) yang bisa dipakai untuk menyusun informasi utama (misalnya SepFormer  achieves  performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sentences ===\n",
      "- Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning.\n",
      "- RNNs, however, are inherently sequential models that do not allow parallelization of their computations.\n",
      "- Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations \n",
      "with a multi-head attention mechanism.\n",
      "- In this paper, we propose the SepFormer, a novel RNN-free\n",
      "Transformer-based neural network for speech separation.\n",
      "- The Sep-Former learns short and long-term\n",
      "dependencies with a multi-scale approach that employs transformers.\n",
      "- The proposed model achieves\n",
      "state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets.\n",
      "- It reaches an SI-SNRi\n",
      "of 22.3 dB on WSJ0-2mix and an SI-SNRi of 19.5 dB on WSJ0-3mix.\n",
      "- The SepFormer inherits the parallelization\n",
      "advantages of Transformers and achieves a competitive performance even when downsampling the encoded\n",
      "representation by a factor of 8.\n",
      "- It is thus significantly faster and it is less memory-demanding\n",
      "than the latest speech separation systems with comparable performance.\n",
      "\n",
      "=== Entities (NER) ===\n",
      "('Recurrent Neural Networks', 'ORG')\n",
      "('SepFormer', 'ORG')\n",
      "('WSJ0-2/3mix', 'EVENT')\n",
      "('22.3', 'CARDINAL')\n",
      "('WSJ0-2mix', 'DATE')\n",
      "('19.5', 'CARDINAL')\n",
      "('WSJ0-3mix', 'DATE')\n",
      "('SepFormer', 'ORG')\n",
      "('8', 'CARDINAL')\n",
      "\n",
      "=== Masked Text ===\n",
      "[ORG] (RNNs) have long been the dominant architecture in sequence-to-sequence learning.\n",
      "RNNs, however, are inherently sequential models that do not allow parallelization of their computations.\n",
      "Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations \n",
      "with a multi-head attention mechanism. In this paper, we propose the [ORG], a novel RNN-free\n",
      "Transformer ...\n",
      "\n",
      "=== Noun Phrases ===\n",
      "['Recurrent Neural Networks', 'RNNs', 'the dominant architecture', 'sequence', 'RNNs', 'sequential models', 'that', 'parallelization', 'their computations', 'Transformers', 'a natural alternative', 'standard RNNs', 'recurrent computations', 'a multi-head attention mechanism', 'this paper']\n",
      "\n",
      "=== Subject-Verb-Object Triplets ===\n",
      "(['Transformers'], 'emerging', ['as'])\n",
      "(['we'], 'propose', ['SepFormer'])\n",
      "(['Former'], 'learns', ['dependencies'])\n",
      "(['model'], 'achieves', ['performance'])\n",
      "(['It'], 'reaches', ['SNRi', 'dB', 'on', 'dB', 'on'])\n",
      "(['SepFormer'], 'inherits', ['advantages'])\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Abstract text\n",
    "text = \"\"\"Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning.\n",
    "RNNs, however, are inherently sequential models that do not allow parallelization of their computations.\n",
    "Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations \n",
    "with a multi-head attention mechanism. In this paper, we propose the SepFormer, a novel RNN-free\n",
    "Transformer-based neural network for speech separation. The Sep-Former learns short and long-term\n",
    "dependencies with a multi-scale approach that employs transformers. The proposed model achieves\n",
    "state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It reaches an SI-SNRi\n",
    "of 22.3 dB on WSJ0-2mix and an SI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the parallelization\n",
    "advantages of Transformers and achieves a competitive performance even when downsampling the encoded\n",
    "representation by a factor of 8. It is thus significantly faster and it is less memory-demanding\n",
    "than the latest speech separation systems with comparable performance.\"\"\"\n",
    "\n",
    "# Process text\n",
    "doc = nlp(text)\n",
    "\n",
    "# 1. Sentence Splitter\n",
    "sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "# 2. Entity Recognition + Masking\n",
    "masked_text = text\n",
    "for ent in doc.ents:\n",
    "    masked_text = masked_text.replace(ent.text, f\"[{ent.label_}]\")\n",
    "\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# 3. Phrase Chunking (noun phrases)\n",
    "noun_chunks = [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "# 4. Dependency Parsing (extract subject-verb-object triplets)\n",
    "triplets = []\n",
    "for token in doc:\n",
    "    if token.dep_ == \"ROOT\" and token.pos_ == \"VERB\":\n",
    "        subj = [w.text for w in token.lefts if w.dep_ in (\"nsubj\", \"nsubjpass\")]\n",
    "        obj = [w.text for w in token.rights if w.dep_ in (\"dobj\", \"attr\", \"prep\", \"pobj\")]\n",
    "        if subj and obj:\n",
    "            triplets.append((subj, token.text, obj))\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"=== Sentences ===\")\n",
    "for s in sentences:\n",
    "    print(\"-\", s)\n",
    "\n",
    "print(\"\\n=== Entities (NER) ===\")\n",
    "for e in entities:\n",
    "    print(e)\n",
    "\n",
    "print(\"\\n=== Masked Text ===\")\n",
    "print(masked_text[:400], \"...\")\n",
    "\n",
    "print(\"\\n=== Noun Phrases ===\")\n",
    "print(noun_chunks[:15])\n",
    "\n",
    "print(\"\\n=== Subject-Verb-Object Triplets ===\")\n",
    "for t in triplets:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations and Failure Cases\n",
    "\n",
    "This section demonstrates the limitations and potential failure cases of each NLP tool to help understand when they might not perform optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenging Text Examples for Testing Tool Limitations:\n",
      "============================================================\n",
      "\n",
      "Informal Social Media:\n",
      "'omg this is sooo good!!! cant believe it... lol  #amazing btw u shld check this out'\n",
      "\n",
      "Missing Punctuation:\n",
      "'the quick brown fox jumps over the lazy dog then it runs away very fast'\n",
      "\n",
      "Technical Jargon:\n",
      "'The ReLU activation function in CNNs outperforms sigmoid in backpropagation optimization'\n",
      "\n",
      "Creative Writing:\n",
      "'The moon whispers secrets to the dancing shadows while time melts like forgotten dreams'\n",
      "\n",
      "Multilingual Mix:\n",
      "'Hello, je suis trs happy today porque es un beautiful da'\n",
      "\n",
      "Poor Formatting:\n",
      "'this   is   poorly    formatted\n",
      "text with\tweird\n",
      "spacing and symbols!!!???'\n",
      "\n",
      "Domain Specific:\n",
      "'Patient presents with acute myocardial infarction post-CABG with elevated troponin-I levels'\n",
      "\n",
      "Very Long Sentence:\n",
      "'The research methodology involved collecting data from multiple sources including primary interviews...'\n"
     ]
    }
   ],
   "source": [
    "# Define challenging text examples for testing limitations\n",
    "challenging_texts = {\n",
    "    \"informal_social_media\": \"omg this is sooo good!!! cant believe it... lol  #amazing btw u shld check this out\",\n",
    "    \"missing_punctuation\": \"the quick brown fox jumps over the lazy dog then it runs away very fast\",\n",
    "    \"technical_jargon\": \"The ReLU activation function in CNNs outperforms sigmoid in backpropagation optimization\",\n",
    "    \"creative_writing\": \"The moon whispers secrets to the dancing shadows while time melts like forgotten dreams\",\n",
    "    \"multilingual_mix\": \"Hello, je suis trs happy today porque es un beautiful da\",\n",
    "    \"poor_formatting\": \"this   is   poorly    formatted\\ntext with\\tweird\\r\\nspacing and symbols!!!???\",\n",
    "    \"domain_specific\": \"Patient presents with acute myocardial infarction post-CABG with elevated troponin-I levels\",\n",
    "    \"very_long_sentence\": \"The research methodology involved collecting data from multiple sources including primary interviews with subject matter experts who had extensive experience in the field secondary literature reviews of peer-reviewed academic journals published within the last five years tertiary analysis of publicly available datasets from government and non-governmental organizations and quaternary validation through statistical modeling techniques that incorporated both parametric and non-parametric approaches to ensure robust and comprehensive findings that could withstand rigorous scientific scrutiny.\",\n",
    "}\n",
    "\n",
    "print(\"Challenging Text Examples for Testing Tool Limitations:\")\n",
    "print(\"=\" * 60)\n",
    "for key, text in challenging_texts.items():\n",
    "    print(f\"\\n{key.replace('_', ' ').title()}:\")\n",
    "    print(f\"'{text[:100]}{'...' if len(text) > 100 else ''}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sentence Splitter Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sentence Splitter Limitations:\n",
      "===================================\n",
      "\n",
      "1. Missing punctuation:\n",
      "Input: the quick brown fox jumps over the lazy dog then it runs away very fast\n",
      "Output sentences: 1\n",
      "  1: 'the quick brown fox jumps over the lazy dog then it runs away very fast'\n",
      "    ISSUE: Should be 2+ sentences but treated as 1\n",
      "\n",
      "2. Social media text:\n",
      "Input: omg this is sooo good!!! cant believe it... lol  #amazing btw u shld check this out\n",
      "Output sentences: 2\n",
      "  1: 'omg this is sooo good!!!'\n",
      "  2: 'cant believe it... lol  #amazing btw u shld check this out'\n",
      "    PARTIAL SUCCESS: Handles some punctuation but misses context\n",
      "\n",
      "3. Poor formatting:\n",
      "Input: this   is   poorly    formatted\n",
      "text with\tweird\n",
      "spacing and symbols!!!???\n",
      "Output sentences: 1\n",
      "  1: 'this   is   poorly    formatted\n",
      "text with\tweird\n",
      "spacing and symbols!!!???'\n",
      "    ISSUE: Poor formatting treated as single sentence\n"
     ]
    }
   ],
   "source": [
    "print(\" Sentence Splitter Limitations:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Test on informal text without proper punctuation\n",
    "print(\"\\n1. Missing punctuation:\")\n",
    "no_punct_sentences = sentence_splitter(challenging_texts[\"missing_punctuation\"], nlp)\n",
    "print(f\"Input: {challenging_texts['missing_punctuation']}\")\n",
    "print(f\"Output sentences: {len(no_punct_sentences)}\")\n",
    "for i, sent in enumerate(no_punct_sentences, 1):\n",
    "    print(f\"  {i}: '{sent}'\")\n",
    "print(\"    ISSUE: Should be 2+ sentences but treated as 1\")\n",
    "\n",
    "print(\"\\n2. Social media text:\")\n",
    "social_sentences = sentence_splitter(challenging_texts[\"informal_social_media\"], nlp)\n",
    "print(f\"Input: {challenging_texts['informal_social_media']}\")\n",
    "print(f\"Output sentences: {len(social_sentences)}\")\n",
    "for i, sent in enumerate(social_sentences, 1):\n",
    "    print(f\"  {i}: '{sent}'\")\n",
    "print(\"    PARTIAL SUCCESS: Handles some punctuation but misses context\")\n",
    "\n",
    "print(\"\\n3. Poor formatting:\")\n",
    "poor_sentences = sentence_splitter(challenging_texts[\"poor_formatting\"], nlp)\n",
    "print(f\"Input: {challenging_texts['poor_formatting']}\")\n",
    "print(f\"Output sentences: {len(poor_sentences)}\")\n",
    "for i, sent in enumerate(poor_sentences, 1):\n",
    "    print(f\"  {i}: '{sent}'\")\n",
    "print(\"    ISSUE: Poor formatting treated as single sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokenization Limitations:\n",
      "==============================\n",
      "\n",
      "1. Multilingual text:\n",
      "Input: Hello, je suis trs happy today porque es un beautiful da\n",
      "Tokens:\n",
      "  'Hello'  - Alpha: True, Stop: False \n",
      "  ','  - Alpha: False, Stop: False \n",
      "  'je'  - Alpha: True, Stop: False  not stop\n",
      "  'suis'  - Alpha: True, Stop: False \n",
      "  'trs'  - Alpha: True, Stop: False \n",
      "  'happy'  - Alpha: True, Stop: False \n",
      "  'today'  - Alpha: True, Stop: False \n",
      "  'porque'  - Alpha: True, Stop: False \n",
      "  'es'  - Alpha: True, Stop: False  not stop\n",
      "  'un'  - Alpha: True, Stop: False  not stop\n",
      "\n",
      "2. Domain-specific medical text:\n",
      "Input: Patient presents with acute myocardial infarction post-CABG with elevated troponin-I levels\n",
      "Challenging tokens:\n",
      "  'myocardial' - Alpha: True, Shape: xxxx  handled well\n",
      "\n",
      "3. Poor formatting:\n",
      "Input: this   is   poorly    formatted\n",
      "text with\tweird\n",
      "s...\n",
      "Space tokens found: 6\n",
      "Total tokens: 22\n",
      "    HANDLES WELL: Excessive spaces properly tokenized\n",
      "    ROBUST: Weird formatting doesn't break tokenization\n",
      "\n",
      "4. Emojis and special characters:\n",
      "    '' - Alpha: False, Shape: \n",
      "    '#' - Alpha: False, Shape: #\n"
     ]
    }
   ],
   "source": [
    "print(\" Tokenization Limitations:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Test on multilingual text\n",
    "multilingual_tokens, _ = tokenization(challenging_texts[\"multilingual_mix\"], nlp)\n",
    "print(\"\\n1. Multilingual text:\")\n",
    "print(f\"Input: {challenging_texts['multilingual_mix']}\")\n",
    "print(\"Tokens:\")\n",
    "for token in multilingual_tokens[:10]:\n",
    "    lang_note = \"\" if token['text'] in ['je', 'suis', 'trs'] else \"\" if token['text'] in ['porque', 'es', 'un', 'da'] else \"\"\n",
    "    stop_issue = \" not stop\" if token['text'] in ['je', 'es', 'un'] and not token['is_stop'] else \"\"\n",
    "    print(f\"  '{token['text']}' {lang_note} - Alpha: {token['is_alpha']}, Stop: {token['is_stop']} {stop_issue}\")\n",
    "\n",
    "# Test on domain-specific text\n",
    "domain_tokens, _ = tokenization(challenging_texts[\"domain_specific\"], nlp)\n",
    "print(\"\\n2. Domain-specific medical text:\")\n",
    "print(f\"Input: {challenging_texts['domain_specific']}\")\n",
    "print(\"Challenging tokens:\")\n",
    "problem_tokens = [t for t in domain_tokens if t['text'] in ['myocardial', 'post-CABG', 'troponin-I']]\n",
    "for token in problem_tokens:\n",
    "    hyphen_issue = \" compound split\" if '-' in token['text'] else \" handled well\"\n",
    "    print(f\"  '{token['text']}' - Alpha: {token['is_alpha']}, Shape: {token['shape']} {hyphen_issue}\")\n",
    "\n",
    "# Test on poor formatting\n",
    "formatting_tokens, _ = tokenization(challenging_texts[\"poor_formatting\"], nlp)\n",
    "print(\"\\n3. Poor formatting:\")\n",
    "print(f\"Input: {challenging_texts['poor_formatting'][:50]}...\")\n",
    "space_tokens = [t for t in formatting_tokens if t['is_space']]\n",
    "print(f\"Space tokens found: {len(space_tokens)}\")\n",
    "print(f\"Total tokens: {len(formatting_tokens)}\")\n",
    "print(\"    HANDLES WELL: Excessive spaces properly tokenized\")\n",
    "print(\"    ROBUST: Weird formatting doesn't break tokenization\")\n",
    "\n",
    "# Test emoji and special characters\n",
    "emoji_tokens, _ = tokenization(challenging_texts[\"informal_social_media\"], nlp)\n",
    "emoji_found = [t for t in emoji_tokens if '' in t['text'] or '#' in t['text']]\n",
    "print(f\"\\n4. Emojis and special characters:\")\n",
    "for token in emoji_found:\n",
    "    print(f\"    '{token['text']}' - Alpha: {token['is_alpha']}, Shape: {token['shape']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stemming Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Stemming Limitations:\n",
      "=========================\n",
      "\n",
      "1. Over-stemming and Under-stemming:\n",
      "       original       stemmed         issue\n",
      "      universal     universal under-stemmed\n",
      "   organization    organizate    acceptable\n",
      " specialization  specializate    acceptable\n",
      "nationalization nationalizate    acceptable\n",
      "        running           run    acceptable\n",
      "            ran           ran under-stemmed\n",
      "         better          bett    acceptable\n",
      "          worse         worse under-stemmed\n",
      "          geese         geese under-stemmed\n",
      "       children      children under-stemmed\n",
      "      SepFormer       sepform    acceptable\n",
      "    Transformer     transform    acceptable\n",
      "parallelization parallelizate    acceptable\n",
      "   optimization    optimizate    acceptable\n",
      "\n",
      "2. Technical terminology stemming:\n",
      "Input: The ReLU activation function in CNNs outperforms sigmoid in backpropagation optimization\n",
      "Technical terms stemmed:\n",
      "  'ReLU'  'relu'\n",
      "  'activation'  'activate'\n",
      "  'function'  'function'\n",
      "  'CNNs'  'cnn'\n",
      "  'outperforms'  'outperform'\n",
      "  'sigmoid'  'sigmoid'\n",
      "  'backpropagation'  'backpropagate'\n",
      "  'optimization'  'optimizate'\n"
     ]
    }
   ],
   "source": [
    "print(\" Stemming Limitations:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Test problematic words for stemming\n",
    "problematic_words = [\n",
    "    \"universal\", \"organization\", \"specialization\", \"nationalization\",\n",
    "    \"running\", \"ran\", \"better\", \"worse\", \"geese\", \"children\",\n",
    "    \"SepFormer\", \"Transformer\", \"parallelization\", \"optimization\"\n",
    "]\n",
    "\n",
    "print(\"\\n1. Over-stemming and Under-stemming:\")\n",
    "stemming_results = []\n",
    "for word in problematic_words:\n",
    "    stemmed = simple_stemmer(word)\n",
    "    stemming_results.append({\n",
    "        \"original\": word,\n",
    "        \"stemmed\": stemmed,\n",
    "        \"issue\": \"over-stemmed\" if len(stemmed) < len(word) - 4 else \n",
    "                \"under-stemmed\" if word == stemmed else \"acceptable\"\n",
    "    })\n",
    "\n",
    "df_stem_issues = pd.DataFrame(stemming_results)\n",
    "print(df_stem_issues.to_string(index=False))\n",
    "\n",
    "# Test on technical text\n",
    "tech_stems = stemming(challenging_texts[\"technical_jargon\"], nlp)\n",
    "print(f\"\\n2. Technical terminology stemming:\")\n",
    "print(f\"Input: {challenging_texts['technical_jargon']}\")\n",
    "print(\"Technical terms stemmed:\")\n",
    "for stem in tech_stems[:8]:\n",
    "    print(f\"  '{stem['original']}'  '{stem['stemmed']}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lemmatization Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Limitations:\n",
      "==============================\n",
      "\n",
      "1. Domain-specific medical terms:\n",
      "Input: Patient presents with acute myocardial infarction post-CABG with elevated troponin-I levels\n",
      "Lemmatization results:\n",
      "  'Patient'  'patient' (ADJ)\n",
      "  'presents'  'present' (NOUN)\n",
      "  'CABG'  'cabg' (ADJ)\n",
      "\n",
      "2. Multilingual text:\n",
      "Input: Hello, je suis trs happy today porque es un beautiful da\n",
      "Problematic lemmatization:\n",
      "  'suis'  'suis' (PROPN)\n",
      "  'trs'  'trs' (PROPN)\n",
      "  'porque'  'porque' (NOUN)\n",
      "  'da'  'da' (NOUN)\n",
      "\n",
      "3. Processing speed comparison on long text:\n",
      "Stemming time: 0.0210 seconds\n",
      "Lemmatization time: 0.0160 seconds\n",
      "Speed difference: 0.8x slower\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatization Limitations:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Test on domain-specific and out-of-vocabulary words\n",
    "domain_lemmas = lemmatization(challenging_texts[\"domain_specific\"], nlp)\n",
    "print(\"\\n1. Domain-specific medical terms:\")\n",
    "print(f\"Input: {challenging_texts['domain_specific']}\")\n",
    "print(\"Lemmatization results:\")\n",
    "for lemma in domain_lemmas[:10]:\n",
    "    if lemma['original'] != lemma['lemma']:\n",
    "        print(f\"  '{lemma['original']}'  '{lemma['lemma']}' ({lemma['pos']})\")\n",
    "\n",
    "# Test on multilingual text\n",
    "multi_lemmas = lemmatization(challenging_texts[\"multilingual_mix\"], nlp)\n",
    "print(f\"\\n2. Multilingual text:\")\n",
    "print(f\"Input: {challenging_texts['multilingual_mix']}\")\n",
    "print(\"Problematic lemmatization:\")\n",
    "for lemma in multi_lemmas:\n",
    "    if lemma['original'] in ['suis', 'trs', 'porque', 'da']:\n",
    "        print(f\"  '{lemma['original']}'  '{lemma['lemma']}' ({lemma['pos']})\")\n",
    "\n",
    "# Test processing speed comparison\n",
    "import time\n",
    "test_text = challenging_texts[\"very_long_sentence\"]\n",
    "\n",
    "start_time = time.time()\n",
    "stemmed_long = stemming(test_text, nlp)\n",
    "stem_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "lemma_long = lemmatization(test_text, nlp)\n",
    "lemma_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n3. Processing speed comparison on long text:\")\n",
    "print(f\"Stemming time: {stem_time:.4f} seconds\")\n",
    "print(f\"Lemmatization time: {lemma_time:.4f} seconds\")\n",
    "print(f\"Speed difference: {lemma_time/stem_time:.1f}x slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entity Masking (NER) Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Entity Masking (NER) Limitations:\n",
      "========================================\n",
      "\n",
      "1. Domain-specific medical text:\n",
      "Input: Patient presents with acute myocardial infarction post-CABG with elevated troponin-I levels\n",
      "Entities found: 0\n",
      "   CONFIRMED LIMITATION: No medical entities detected!\n",
      "\n",
      "2. Creative/metaphorical text:\n",
      "Input: The moon whispers secrets to the dancing shadows while time melts like forgotten dreams\n",
      "Entities found: 0\n",
      "   ACTUALLY GOOD: No false positives on metaphorical text\n",
      "\n",
      "3. Informal social media text:\n",
      "Input: omg this is sooo good!!! cant believe it... lol  #amazing btw u shld check this out\n",
      "Entities found: 0\n",
      "   ACTUALLY GOOD: No false positives on informal text\n",
      "\n",
      "4. Context ambiguity examples:\n",
      "Input: I went to Turkey and saw apple trees. Will Smith was very good in that movie.\n",
      "Context-dependent classifications:\n",
      "  'Turkey'  GPE (correctly identified as country)\n",
      "  'Will Smith'  PERSON (correctly identified as person)\n"
     ]
    }
   ],
   "source": [
    "print(\" Entity Masking (NER) Limitations:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test on domain-specific entities\n",
    "domain_entities, domain_masked = entity_masking(challenging_texts[\"domain_specific\"], nlp)\n",
    "print(\"\\n1. Domain-specific medical text:\")\n",
    "print(f\"Input: {challenging_texts['domain_specific']}\")\n",
    "print(f\"Entities found: {len(domain_entities)}\")\n",
    "if domain_entities:\n",
    "    for ent in domain_entities:\n",
    "        print(f\"  '{ent['text']}'  {ent['label']} ({ent['description']})\")\n",
    "else:\n",
    "    print(\"   CONFIRMED LIMITATION: No medical entities detected!\")\n",
    "\n",
    "# Test on creative/metaphorical text\n",
    "creative_entities, creative_masked = entity_masking(challenging_texts[\"creative_writing\"], nlp)\n",
    "print(f\"\\n2. Creative/metaphorical text:\")\n",
    "print(f\"Input: {challenging_texts['creative_writing']}\")\n",
    "print(f\"Entities found: {len(creative_entities)}\")\n",
    "if creative_entities:\n",
    "    for ent in creative_entities:\n",
    "        print(f\"  '{ent['text']}'  {ent['label']} ({ent['description']})\")\n",
    "else:\n",
    "    print(\"   ACTUALLY GOOD: No false positives on metaphorical text\")\n",
    "\n",
    "# Test on informal social media text\n",
    "social_entities, social_masked = entity_masking(challenging_texts[\"informal_social_media\"], nlp)\n",
    "print(f\"\\n3. Informal social media text:\")\n",
    "print(f\"Input: {challenging_texts['informal_social_media']}\")\n",
    "print(f\"Entities found: {len(social_entities)}\")\n",
    "if social_entities:\n",
    "    for ent in social_entities:\n",
    "        print(f\"  '{ent['text']}'  {ent['label']} ({ent['description']})\")\n",
    "else:\n",
    "    print(\"   ACTUALLY GOOD: No false positives on informal text\")\n",
    "\n",
    "# Test false positives - common words misclassified\n",
    "false_positive_text = \"I went to Turkey and saw apple trees. Will Smith was very good in that movie.\"\n",
    "fp_entities, fp_masked = entity_masking(false_positive_text, nlp)\n",
    "print(f\"\\n4. Context ambiguity examples:\")\n",
    "print(f\"Input: {false_positive_text}\")\n",
    "print(\"Context-dependent classifications:\")\n",
    "for ent in fp_entities:\n",
    "    context = \"country\" if ent['text'] == 'Turkey' else \"person\"\n",
    "    print(f\"  '{ent['text']}'  {ent['label']} (correctly identified as {context})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. POS Tagging Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " POS Tagging Limitations:\n",
      "==============================\n",
      "\n",
      "1. Ambiguous word contexts:\n",
      "\n",
      "1. 'The duck is ready to eat.'\n",
      "    'duck'  NOUN (noun)\n",
      "\n",
      "2. 'Time flies like an arrow.'\n",
      "    'flies'  VERB (verb)\n",
      "    'like'  ADP (adposition)\n",
      "\n",
      "3. 'The ship was sailed by the harbor.'\n",
      "    'sailed'  VERB (verb)\n",
      "\n",
      "4. 'I can can the can.'\n",
      "    'can'  AUX (auxiliary)\n",
      "    'can'  AUX (auxiliary)\n",
      "    'can'  NOUN (noun)\n",
      "\n",
      "2. Out-of-vocabulary words:\n",
      "Input: The chatbot utilizez machine-learning algorithms to maximizez performance.\n",
      "OOV word tagging:\n",
      "    'utilizez'  NOUN (should be VERB)\n",
      "    'maximizez'  NOUN (should be VERB)\n",
      "\n",
      "3. Informal social media text:\n",
      "Input: omg this is sooo good!!! cant believe it... lol  #amazing btw u shld check this out\n",
      "Challenging POS tags:\n",
      "    'omg'  INTJ (interjection)\n",
      "    'sooo'  ADJ (adjective)\n",
      "    'lol'  X (other)\n",
      "    'btw'  PROPN (proper noun)\n",
      "    'shld'  NOUN (noun)\n",
      "\n",
      "4. Technical jargon:\n",
      "Input: The ReLU activation function in CNNs outperforms sigmoid in backpropagation optimization\n",
      "Technical term POS tags:\n",
      "    'ReLU'  NOUN (correctly as noun)\n",
      "    'CNNs'  NOUN (correctly as noun)\n",
      "    'backpropagation'  NOUN (correctly as noun)\n"
     ]
    }
   ],
   "source": [
    "print(\" POS Tagging Limitations:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Test ambiguous words\n",
    "ambiguous_sentences = [\n",
    "    \"The duck is ready to eat.\",  # duck = noun vs verb\n",
    "    \"Time flies like an arrow.\",  # flies = noun vs verb, like = verb vs preposition\n",
    "    \"The ship was sailed by the harbor.\",  # sailed = past tense vs past participle\n",
    "    \"I can can the can.\",  # multiple uses of 'can'\n",
    "]\n",
    "\n",
    "print(\"\\n1. Ambiguous word contexts:\")\n",
    "for i, sentence in enumerate(ambiguous_sentences, 1):\n",
    "    pos_result = pos_tagging(sentence, nlp)\n",
    "    print(f\"\\n{i}. '{sentence}'\")\n",
    "    ambiguous_words = ['duck', 'flies', 'like', 'sailed', 'can']\n",
    "    for token in pos_result:\n",
    "        if token['text'].lower() in ambiguous_words:\n",
    "            correctness = \"\" if (token['text'] == 'duck' and token['pos'] == 'NOUN') or \\\n",
    "                                 (token['text'] == 'flies' and token['pos'] == 'VERB') else \"\"\n",
    "            print(f\"   {correctness} '{token['text']}'  {token['pos']} ({token['pos_description']})\")\n",
    "\n",
    "# Test out-of-vocabulary words\n",
    "oov_text = \"The chatbot utilizez machine-learning algorithms to maximizez performance.\"\n",
    "oov_pos = pos_tagging(oov_text, nlp)\n",
    "print(f\"\\n2. Out-of-vocabulary words:\")\n",
    "print(f\"Input: {oov_text}\")\n",
    "print(\"OOV word tagging:\")\n",
    "for token in oov_pos:\n",
    "    if token['text'] in ['utilizez', 'machine-learning', 'maximizez']:\n",
    "        correctness = \"\" if token['pos'] == 'NOUN' else \"\"\n",
    "        expected = \"VERB\" if 'ez' in token['text'] else \"NOUN\"\n",
    "        print(f\"   {correctness} '{token['text']}'  {token['pos']} (should be {expected})\")\n",
    "\n",
    "# Test informal social media language\n",
    "informal_pos = pos_tagging(challenging_texts[\"informal_social_media\"], nlp)\n",
    "print(f\"\\n3. Informal social media text:\")\n",
    "print(f\"Input: {challenging_texts['informal_social_media']}\")\n",
    "print(\"Challenging POS tags:\")\n",
    "informal_words = ['omg', 'sooo', 'cant', 'lol', 'btw', 'shld']\n",
    "for token in informal_pos:\n",
    "    if any(word in token['text'].lower() for word in informal_words):\n",
    "        accuracy = \"\" if token['pos'] in ['INTJ', 'ADJ', 'X'] else \"\"\n",
    "        print(f\"   {accuracy} '{token['text']}'  {token['pos']} ({token['pos_description']})\")\n",
    "\n",
    "# Test very technical jargon\n",
    "tech_pos = pos_tagging(challenging_texts[\"technical_jargon\"], nlp)\n",
    "print(f\"\\n4. Technical jargon:\")\n",
    "print(f\"Input: {challenging_texts['technical_jargon']}\")\n",
    "print(\"Technical term POS tags:\")\n",
    "tech_terms = ['ReLU', 'CNNs', 'backpropagation']\n",
    "for token in tech_pos:\n",
    "    if token['text'] in tech_terms:\n",
    "        print(f\"    '{token['text']}'  {token['pos']} (correctly as {token['pos_description']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Phrase Chunking Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Phrase Chunking Limitations:\n",
      "===================================\n",
      "\n",
      "1. Complex nested phrases:\n",
      "\n",
      "1. 'The big red car that my friend bought yesterday was expensive.'\n",
      "   Noun phrases found: 3\n",
      "   - 'The big red car' (root: car)\n",
      "   - 'that' (root: that)\n",
      "   - 'my friend' (root: friend)\n",
      "    ISSUE: 'big red car that my friend bought yesterday' broken into fragments\n",
      "\n",
      "2. 'Students who study hard in the library during exam week usually succeed.'\n",
      "   Noun phrases found: 4\n",
      "   - 'Students' (root: Students)\n",
      "   - 'who' (root: who)\n",
      "   - 'the library' (root: library)\n",
      "   - 'exam week' (root: week)\n",
      "\n",
      "3. 'The machine learning algorithm developed by researchers at Stanford University performs well.'\n",
      "   Noun phrases found: 4\n",
      "   - 'The machine' (root: machine)\n",
      "   - 'algorithm' (root: algorithm)\n",
      "   - 'researchers' (root: researchers)\n",
      "   - 'Stanford University' (root: University)\n",
      "    ISSUE: 'machine learning algorithm' split incorrectly\n",
      "\n",
      "2. Informal social media text:\n",
      "Input: omg this is sooo good!!! cant believe it... lol  #amazing btw u shld check this out\n",
      "Noun phrases: 5\n",
      "   - 'this'\n",
      "   - 'it'\n",
      "   - 'lol  #'\n",
      "   - 'btw u shld'\n",
      "   - 'this'\n",
      "Verb phrases: 2\n",
      "   - 'believe it'\n",
      "   - 'check this'\n",
      "    MIXED: Some good chunks ('believe it'), some odd ('btw u shld')\n",
      "\n",
      "3. Creative/metaphorical text:\n",
      "Input: The moon whispers secrets to the dancing shadows while time melts like forgotten dreams\n",
      "Metaphorical phrases detected:\n",
      "    'The moon' (correctly chunked despite metaphor)\n",
      "    'the dancing shadows' (correctly chunked despite metaphor)\n",
      "    'forgotten dreams' (correctly chunked despite metaphor)\n",
      "\n",
      "4. Complex verb phrase coordination:\n",
      "Input: The system processes data, analyzes patterns, generates reports, and provides insights.\n",
      "Verb phrases found: 3\n",
      "Coordinated verbs captured:\n",
      "    'processes data'\n",
      "    'generates reports'\n",
      "    'provides insights'\n",
      "    MISSED: 'analyzes patterns' - coordination not fully captured\n"
     ]
    }
   ],
   "source": [
    "print(\" Phrase Chunking Limitations:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Test complex nested phrases\n",
    "complex_sentences = [\n",
    "    \"The big red car that my friend bought yesterday was expensive.\",\n",
    "    \"Students who study hard in the library during exam week usually succeed.\",\n",
    "    \"The machine learning algorithm developed by researchers at Stanford University performs well.\",\n",
    "]\n",
    "\n",
    "print(\"\\n1. Complex nested phrases:\")\n",
    "for i, sentence in enumerate(complex_sentences, 1):\n",
    "    phrases = phrase_chunking(sentence, nlp)\n",
    "    print(f\"\\n{i}. '{sentence}'\")\n",
    "    print(f\"   Noun phrases found: {len(phrases['noun_phrases'])}\")\n",
    "    for np in phrases['noun_phrases'][:5]:  # Show first 5\n",
    "        print(f\"   - '{np['text']}' (root: {np['root']})\")\n",
    "    if i == 1:\n",
    "        print(\"    ISSUE: 'big red car that my friend bought yesterday' broken into fragments\")\n",
    "    elif i == 3:\n",
    "        print(\"    ISSUE: 'machine learning algorithm' split incorrectly\")\n",
    "\n",
    "# Test on informal text\n",
    "informal_phrases = phrase_chunking(challenging_texts[\"informal_social_media\"], nlp)\n",
    "print(f\"\\n2. Informal social media text:\")\n",
    "print(f\"Input: {challenging_texts['informal_social_media']}\")\n",
    "print(f\"Noun phrases: {len(informal_phrases['noun_phrases'])}\")\n",
    "for np in informal_phrases['noun_phrases']:\n",
    "    print(f\"   - '{np['text']}'\")\n",
    "print(f\"Verb phrases: {len(informal_phrases['verb_phrases'])}\")\n",
    "for vp in informal_phrases['verb_phrases']:\n",
    "    print(f\"   - '{vp['text']}'\")\n",
    "print(\"    MIXED: Some good chunks ('believe it'), some odd ('btw u shld')\")\n",
    "\n",
    "# Test on creative writing\n",
    "creative_phrases = phrase_chunking(challenging_texts[\"creative_writing\"], nlp)\n",
    "print(f\"\\n3. Creative/metaphorical text:\")\n",
    "print(f\"Input: {challenging_texts['creative_writing']}\")\n",
    "print(\"Metaphorical phrases detected:\")\n",
    "for np in creative_phrases['noun_phrases']:\n",
    "    if any(word in np['text'].lower() for word in ['moon', 'shadows', 'time', 'dreams']):\n",
    "        print(f\"    '{np['text']}' (correctly chunked despite metaphor)\")\n",
    "\n",
    "# Test incomplete verb phrases\n",
    "incomplete_vp_text = \"The system processes data, analyzes patterns, generates reports, and provides insights.\"\n",
    "incomplete_phrases = phrase_chunking(incomplete_vp_text, nlp)\n",
    "print(f\"\\n4. Complex verb phrase coordination:\")\n",
    "print(f\"Input: {incomplete_vp_text}\")\n",
    "print(f\"Verb phrases found: {len(incomplete_phrases['verb_phrases'])}\")\n",
    "print(\"Coordinated verbs captured:\")\n",
    "for vp in incomplete_phrases['verb_phrases']:\n",
    "    print(f\"    '{vp['text']}'\")\n",
    "print(\"    MISSED: 'analyzes patterns' - coordination not fully captured\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Syntactic Parser Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Parser Limitations:\n",
      "===================================\n",
      "\n",
      "1. Very long complex sentence:\n",
      "Input length: 596 characters\n",
      "Dependencies found: 80\n",
      "Complex dependency chain (first 10):\n",
      "   'The' --det--> 'methodology'\n",
      "   'research' --compound--> 'methodology'\n",
      "   'methodology' --nsubj--> 'involved'\n",
      "   'involved' --ROOT--> 'involved'\n",
      "   'collecting' --xcomp--> 'involved'\n",
      "   'data' --dobj--> 'collecting'\n",
      "   'from' --prep--> 'collecting'\n",
      "   'multiple' --amod--> 'sources'\n",
      "   'sources' --pobj--> 'from'\n",
      "   'including' --prep--> 'sources'\n",
      "\n",
      "2. Performance comparison:\n",
      "Simple sentence (24 chars): 0.0050s\n",
      "Complex sentence (596 chars): 0.0160s\n",
      "Performance degradation: 3.2x slower\n",
      "\n",
      "3. Ambiguous sentence structures:\n",
      "\n",
      "1. 'I saw the man with the telescope.'\n",
      "   Root: 'saw'\n",
      "   Subjects: ['I']\n",
      "   Objects: ['man', 'telescope']\n",
      "   Key dependencies:\n",
      "     'saw' --ROOT--> 'saw'\n",
      "     'telescope' --pobj--> 'with'\n",
      "\n",
      "2. 'Flying planes can be dangerous.'\n",
      "   Root: 'be'\n",
      "   Subjects: ['planes']\n",
      "   Objects: []\n",
      "   Key dependencies:\n",
      "     'Flying' --amod--> 'planes'\n",
      "     'planes' --nsubj--> 'be'\n",
      "\n",
      "3. 'The chicken is ready to eat.'\n",
      "   Root: 'is'\n",
      "   Subjects: ['chicken']\n",
      "   Objects: []\n",
      "   Key dependencies:\n",
      "     'chicken' --nsubj--> 'is'\n",
      "     'eat' --xcomp--> 'ready'\n",
      "\n",
      "4. Poor formatting and grammar:\n",
      "Input: this   is   poorly    formatted\n",
      "text with\tweird\n",
      "spacing and symbols!!!???\n",
      "Dependencies: 16\n",
      "Parsing challenges:\n",
      "   Sentence root: 'is' (AUX)\n"
     ]
    }
   ],
   "source": [
    "print(\"Syntactic Parser Limitations:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Test very long complex sentence\n",
    "long_deps, long_structures = syntactic_parsing(challenging_texts[\"very_long_sentence\"], nlp)\n",
    "print(\"\\n1. Very long complex sentence:\")\n",
    "print(f\"Input length: {len(challenging_texts['very_long_sentence'])} characters\")\n",
    "print(f\"Dependencies found: {len(long_deps)}\")\n",
    "print(\"Complex dependency chain (first 10):\")\n",
    "for dep in long_deps[:10]:\n",
    "    print(f\"   '{dep['text']}' --{dep['dep']}--> '{dep['head']}'\")\n",
    "\n",
    "# Test parsing performance on complex vs simple sentences\n",
    "simple_sentence = \"The cat sits on the mat.\"\n",
    "complex_sentence = challenging_texts[\"very_long_sentence\"]\n",
    "\n",
    "import time\n",
    "\n",
    "# Simple sentence parsing\n",
    "start_time = time.time()\n",
    "simple_deps, simple_structures = syntactic_parsing(simple_sentence, nlp)\n",
    "simple_time = time.time() - start_time\n",
    "\n",
    "# Complex sentence parsing\n",
    "start_time = time.time()\n",
    "complex_deps, complex_structures = syntactic_parsing(complex_sentence, nlp)\n",
    "complex_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n2. Performance comparison:\")\n",
    "print(f\"Simple sentence ({len(simple_sentence)} chars): {simple_time:.4f}s\")\n",
    "print(f\"Complex sentence ({len(complex_sentence)} chars): {complex_time:.4f}s\")\n",
    "print(f\"Performance degradation: {complex_time/simple_time:.1f}x slower\")\n",
    "\n",
    "# Test on ambiguous sentence structures\n",
    "ambiguous_structures = [\n",
    "    \"I saw the man with the telescope.\",  # Attachment ambiguity\n",
    "    \"Flying planes can be dangerous.\",    # Syntactic ambiguity\n",
    "    \"The chicken is ready to eat.\",       # Semantic ambiguity affects parsing\n",
    "]\n",
    "\n",
    "print(f\"\\n3. Ambiguous sentence structures:\")\n",
    "for i, sentence in enumerate(ambiguous_structures, 1):\n",
    "    deps, structures = syntactic_parsing(sentence, nlp)\n",
    "    print(f\"\\n{i}. '{sentence}'\")\n",
    "    if structures:\n",
    "        struct = structures[0]\n",
    "        print(f\"   Root: '{struct['root']}'\")\n",
    "        print(f\"   Subjects: {struct['subjects']}\")\n",
    "        print(f\"   Objects: {struct['objects']}\")\n",
    "        print(\"   Key dependencies:\")\n",
    "        key_deps = [d for d in deps if d['text'] in ['saw', 'telescope', 'Flying', 'planes', 'chicken', 'eat']]\n",
    "        for dep in key_deps[:3]:\n",
    "            print(f\"     '{dep['text']}' --{dep['dep']}--> '{dep['head']}'\")\n",
    "\n",
    "# Test on informal/broken grammar\n",
    "informal_deps, informal_structures = syntactic_parsing(challenging_texts[\"poor_formatting\"], nlp)\n",
    "print(f\"\\n4. Poor formatting and grammar:\")\n",
    "print(f\"Input: {challenging_texts['poor_formatting']}\")\n",
    "print(f\"Dependencies: {len(informal_deps)}\")\n",
    "print(\"Parsing challenges:\")\n",
    "for struct in informal_structures:\n",
    "    print(f\"   Sentence root: '{struct['root']}' ({struct['root_pos']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
